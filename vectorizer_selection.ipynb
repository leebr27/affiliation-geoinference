{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Suppress \"SettingWithCopyWarning\"\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_spacy_mapaffil = pd.read_parquet(\"data/clean_spacy_mapaffil.parquet\", engine=\"fastparquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affiliations = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_spacy_mapaffil.head(num_affiliations)\n",
    "city_counts = df['city'].value_counts()\n",
    "single_instance_cities = city_counts[city_counts == 1].index.tolist()\n",
    "num_affiliations -= len(single_instance_cities)\n",
    "filtered_df = df[~df['city'].isin(single_instance_cities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['city'] = filtered_df['city'].astype('category')\n",
    "filtered_df['label'] = filtered_df['city'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_test_size = (filtered_df['city'].nunique()) / num_affiliations\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(filtered_df[\"affiliation\"], filtered_df[\"city\"], test_size=calculated_test_size if calculated_test_size > 0.1 else 0.1, stratify=filtered_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer \n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", decode_error=\"ignore\") \n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_texts)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Vectorizer\n",
    "X_train_texts_processed = X_train_texts.apply(gensim.utils.simple_preprocess)\n",
    "X_test_texts_processed = X_test_texts.apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "model = gensim.models.Word2Vec(window=10, min_count=2, workers=4)\n",
    "\n",
    "model.build_vocab(X_train_texts_processed, progress_per=100)\n",
    "model.train(X_train_texts_processed, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "def vectorize_text(text, model):\n",
    "    vectors = []\n",
    "    for word in text:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "X_train_w2v = np.array([vectorize_text(text, model) for text in X_train_texts_processed]) \n",
    "X_test_w2v = np.array([vectorize_text(text, model) for text in X_test_texts_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Vectorizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def embed_texts_bert(texts):\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :].numpy()  \n",
    "    return embeddings\n",
    "\n",
    "X_train_bert = embed_texts_bert(list(X_train_texts))\n",
    "X_test_bert = embed_texts_bert(list(X_test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_test, y_train, y_test, vectorizer_name, clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate TF-IDF\n",
    "accuracy_tfidf_LogisticRegression = train_and_evaluate(X_train_tfidf, X_test_tfidf, y_train, y_test, 'TF-IDF', LogisticRegression())\n",
    "accuracy_tfidf_LinearSVC = train_and_evaluate(X_train_tfidf, X_test_tfidf, y_train, y_test, 'TF-IDF', svm.LinearSVC(dual=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Word2Vec\n",
    "accuracy_w2v_LogisticRegression = train_and_evaluate(X_train_w2v, X_test_w2v, y_train, y_test, 'Word2Vec', LogisticRegression())\n",
    "accuracy_w2v_LinearSVC = train_and_evaluate(X_train_w2v, X_test_w2v, y_train, y_test, 'Word2Vec', svm.LinearSVC(dual=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BERT\n",
    "accuracy_bert_LogisticRegression = train_and_evaluate(X_train_bert, X_test_bert, y_train, y_test, 'BERT', LogisticRegression())\n",
    "accuracy_bert_LinearSVC = train_and_evaluate(X_train_bert, X_test_bert, y_train, y_test, 'BERT', svm.LinearSVC(dual=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Vectorizer': ['TF-IDF (Logistic Regression)', 'Word2Vec (Logistic Regression)', 'BERT (Logistic Regression)', 'TF-IDF (LinearSVC)', 'Word2Vec (LinearSVC)', 'BERT (LinearSVC)'],\n",
    "    'Accuracy': [accuracy_tfidf_LogisticRegression, accuracy_w2v_LogisticRegression, accuracy_bert_LogisticRegression, accuracy_tfidf_LinearSVC, accuracy_w2v_LinearSVC, accuracy_bert_LinearSVC]\n",
    "})\n",
    "\n",
    "display(results)\n",
    "\n",
    "print(f\"Total number of affiliations in current dataset: {num_affiliations}\")\n",
    "print(f\"Test Size: {calculated_test_size if calculated_test_size > 0.1 else 0.1}\")\n",
    "print(f\"# of training affiliations: {num_affiliations - int(calculated_test_size * num_affiliations)}\")\n",
    "print(f\"# of test affiliations: {int(calculated_test_size * num_affiliations)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
